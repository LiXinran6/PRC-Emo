## Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning
ðŸŽ‰ Accepted at AAAI 2026 (Main Track)

Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speakerâ€™s psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasetsâ€”IEMOCAP and MELDâ€”show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding.


## Reference & Acknowledgement
This project draws inspiration from the BiosERC framework, which explores integrating speaker biographical information for enhancing ERC tasks.
Please consider citing their work if you use similar ideas:

Xue, Jieying et al. â€œBiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks.â€ International Conference on Artificial Neural Networks (2024).


@InProceedings{10.1007/978-3-031-72344-5_19,
    author    = "Xue, Jieying and Nguyen, Minh-Phuong and Matheny, Blake and Nguyen, Le-Minh",
    editor    = "Wand, Michael and Malinovsk{\'a}, Krist{\'i}na and Schmidhuber, J{\"u}rgen and Tetko, Igor V.",
    title     = "BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks",
    booktitle = "Artificial Neural Networks and Machine Learning -- ICANN 2024",
    year      = "2024",
    publisher = "Springer Nature Switzerland",
    address   = "Cham",
    pages     = "277--292",
    isbn      = "978-3-031-72344-5",
    abstract  = "In the Emotion Recognition in Conversation task, recent investigations have utilized attention mechanisms exploring relationships among utterances from intra- and inter-speakers for modeling emotional interaction between them. However, attributes such as speaker personality traits remain unexplored and present challenges in terms of their applicability to other tasks or compatibility with diverse model architectures. Therefore, this work introduces a novel framework named BiosERC, which investigates speaker characteristics in a conversation. By employing Large Language Models (LLMs), we extract the ``biographical information'' of the speaker within a conversation as supplementary knowledge injected into the model to classify emotional labels for each utterance. Our proposed method achieved state-of-the-art (SOTA) results on three famous benchmark datasets: IEMOCAP, MELD, and EmoryNLP, demonstrating the effectiveness and generalization of our model and showcasing its potential for adaptation to various conversation analysis tasks."
}


##  Data  
- IEMOCAP
    Data structure examples: 
    ```json
    {
        # this is first conversation 
        "Ses05M_impro03": { 
            "labels": [
            4,
            2,
            4,
            4 
            ],
            "sentences": [
            "Guess what?",
            "what?",
            "I did it, I asked her to marry me.",
            "Yes, I did it."
            ], 
            "genders": [
            "M",
            "F",
            "M",
            "M",
            "F", 
            ]
        },

        # this is second conversation 
        "Ses05M_impro03": { 
            "labels": [
            4,
            2,
            ],
            "sentences": [
            "Guess what?",
            "what?", 
            ], 
            "genders": [
            "M",
            "F",  
            ]
        }
    }
    ```

##  Python ENV 
Init python environment 
```cmd
    conda create --prefix=./env_py38  python=3.9
    conda activate ./env_py38 
    pip install -r requirements.txt
```

## Run 
1. Init environment follow the above step.
2. Train  
    Run following command to train a new model. 
    ```bash 
    python src/get_rag_final.py # to get demonstration retrieval repository
    python src/llm_bio_extract_v2.py # to extract speaker bio
    python src/llm_emotion_extract_v2.py # to explicit and implicit emotion interpretations

    bash scripts/train_llm.sh # to train a llm model, or HF_ENDPOINT=https://hf-mirror.com bash scripts/train_llm.sh
    ```
    > **Note**: Please check this scripts to check the setting and choose which data you want to run. For IEMOCAP, MODEL_ID="Qwen2.5-7B-Instruct"; For MELD, MODEL_ID="Qwen3-8B"
